{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Challenge: Build a large-scale image search engine!\n",
    "\n",
    "You and your team of **three Cornell Tech students** are surely on the path to fame and fortune! You have been recruited by Google to disrupt Google Image Search by building a better search engine using novel statistical learning techniques.\n",
    "\n",
    "The specifications are simple: We need a way to **search for relevant images** given a natural language query. For instance, if a user types \"dog jumping to catch frisbee,\" your system will **rank-order the most relevant images** from a large database.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**During training**, you have a dataset of 10,000 samples. \n",
    "\n",
    "Each sample has the following data available for learning:\n",
    "- A 224x224 JPG image.\n",
    "- A list of tags indicating objects appeared in the image.\n",
    "- Feature vectors extracted using [Resnet](https://arxiv.org/abs/1512.03385), a state-of-the-art Deep-learned CNN (You don't have to train or run ResNet -- we are providing the features for you). See [here](http://ethereon.github.io/netscope/#/gist/b21e2aae116dc1ac7b50) for the illustration of the ResNet-101 architecture. The features are extracted from pool5 and fc1000 layer.\n",
    "- A five-sentence description, used to train your search engine.\n",
    "\n",
    "**During testing**, your system matches a single five-sentence description against a pool of 2,000 candidate samples from the test set. \n",
    "\n",
    "Each sample has:\n",
    "- A 224x224 JPEG image.\n",
    "- A list of tags for that image.\n",
    "- ResNet feature vectors for that image.\n",
    "\n",
    "**Output**:\n",
    "For each description, your system must rank-score each testing image with the likelihood of that image matches the given sentence. Your system then returns the name of the top 20 relevant images, delimited by space. See \"sample_submission.csv\" on the data page for more details on the output format.\n",
    "\n",
    "**Evaluation metric**:\n",
    "There are 2,000 descriptions, and for each description, you must compare against the entire 2,000-image test set. That is, rank-order test images for each test description. We will use **MAP@20** as the evaluation metric. If the corresponding image of a description is among your algorithm's 20 highest scoring images, this metric gives you a certain score based on the ranking of the corresponding image. Please refer to the evaluation page for more details. Use all of your skills, tools, and experience. It is OK to use libraries like numpy, scikit-learn, pandas, etc., as long as you cite them. Use cross-validation on training set to debug your algorithm. Submit your results to the Kaggle leaderboard and send your complete writeup to CMS. The data you use --- and the way you use the data --- is completely up to you.\n",
    "\n",
    "**Note**:\n",
    "The best teams of **three Cornell Tech students** might use visualization techniques for debugging (e.g., show top images retrieved by your algorithm and see whether they make sense or not), preprocessing, a nice way to compare tags and descriptions, leveraging visual features and combining them with tags and descriptions, supervised and/or unsupervised learning to best understand how to best take advantage of each data source available to them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**File descriptions**:\n",
    "\n",
    "- images_train - 10,000 training images of size 224x224.\n",
    "- images_test - 2,000 test images of size 224x224.\n",
    "- tags_train - image tags correspond to training images. Each image have several tags indicating the human-labeled object categories appear in the image, in the form of \"supercategory:category\".\n",
    "- tags_test - image tags correspond to test images. Each image have several tags indicating the human-labeled object categories appear in the image, in the form of \"supercategory:category\".\n",
    "features_train - features extracted from a pre-trained Residual Network (ResNet) on training set, including 1,000 dimensional feature from classification layer (fc1000) and 2,048 dimensional feature from final convolution layer (pool5). Each dimension of the fc1000 feature corresponds to a WordNet synset here.\n",
    "- features_test - features extracted from the same Residual Network (ResNet) on test set, including 1,000 dimensional feature from classification layer (fc1000) and 2,048 dimensional feature from final convolution layer (pool5).\n",
    "- descriptions_train - image descriptions correspond to training images. Each image have 5 sentences for describing the image content.\n",
    "- descriptions_test - image descriptions for test images. Each image have 5 sentences for describing the image content. Notice that one test description corresponds to one test image. The task you need to do is to return top 20 images in test set for each test description.\n",
    "- sample_submission.csv - a sample submission file in the correct format.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import pylab as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in training data\n",
    "\n",
    "# descriptions_train\n",
    "# tags_train\n",
    "# features_train\n",
    "# images_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in testing data\n",
    "\n",
    "# descriptions_test\n",
    "# tags_test\n",
    "# features_test\n",
    "# images_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output results to CSV\n",
    "\n",
    "# with open('test_results.csv', 'w') as csvfile:\n",
    "#     writer = csv.writer(csvfile, delimiter=',')\n",
    "#     writer.writerow(['Description_ID','Top_20_Image_IDs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
