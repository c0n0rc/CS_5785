{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Challenge: Build a large-scale image search engine!\n",
    "\n",
    "You and your team of **three Cornell Tech students** are surely on the path to fame and fortune! You have been recruited by Google to disrupt Google Image Search by building a better search engine using novel statistical learning techniques.\n",
    "\n",
    "The specifications are simple: We need a way to **search for relevant images** given a natural language query. For instance, if a user types \"dog jumping to catch frisbee,\" your system will **rank-order the most relevant images** from a large database.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**During training**, you have a dataset of 10,000 samples. \n",
    "\n",
    "Each sample has the following data available for learning:\n",
    "- A 224x224 JPG image.\n",
    "- A list of tags indicating objects appeared in the image.\n",
    "- Feature vectors extracted using [Resnet](https://arxiv.org/abs/1512.03385), a state-of-the-art Deep-learned CNN (You don't have to train or run ResNet -- we are providing the features for you). See [here](http://ethereon.github.io/netscope/#/gist/b21e2aae116dc1ac7b50) for the illustration of the ResNet-101 architecture. The features are extracted from pool5 and fc1000 layer.\n",
    "- A five-sentence description, used to train your search engine.\n",
    "\n",
    "**During testing**, your system matches a single five-sentence description against a pool of 2,000 candidate samples from the test set. \n",
    "\n",
    "Each sample has:\n",
    "- A 224x224 JPEG image.\n",
    "- A list of tags for that image.\n",
    "- ResNet feature vectors for that image.\n",
    "\n",
    "**Output**:\n",
    "For each description, your system must rank-score each testing image with the likelihood of that image matches the given sentence. Your system then returns the name of the top 20 relevant images, delimited by space. See \"sample_submission.csv\" on the data page for more details on the output format.\n",
    "\n",
    "**Evaluation metric**:\n",
    "There are 2,000 descriptions, and for each description, you must compare against the entire 2,000-image test set. That is, rank-order test images for each test description. We will use **MAP@20** as the evaluation metric. If the corresponding image of a description is among your algorithm's 20 highest scoring images, this metric gives you a certain score based on the ranking of the corresponding image. Please refer to the evaluation page for more details. Use all of your skills, tools, and experience. It is OK to use libraries like numpy, scikit-learn, pandas, etc., as long as you cite them. Use cross-validation on training set to debug your algorithm. Submit your results to the Kaggle leaderboard and send your complete writeup to CMS. The data you use --- and the way you use the data --- is completely up to you.\n",
    "\n",
    "**Note**:\n",
    "The best teams of **three Cornell Tech students** might use visualization techniques for debugging (e.g., show top images retrieved by your algorithm and see whether they make sense or not), preprocessing, a nice way to compare tags and descriptions, leveraging visual features and combining them with tags and descriptions, supervised and/or unsupervised learning to best understand how to best take advantage of each data source available to them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**File descriptions**:\n",
    "\n",
    "- images_train - 10,000 training images of size 224x224.\n",
    "- images_test - 2,000 test images of size 224x224.\n",
    "- tags_train - image tags correspond to training images. Each image have several tags indicating the human-labeled object categories appear in the image, in the form of \"supercategory:category\".\n",
    "- tags_test - image tags correspond to test images. Each image have several tags indicating the human-labeled object categories appear in the image, in the form of \"supercategory:category\".\n",
    "features_train - features extracted from a pre-trained Residual Network (ResNet) on training set, including 1,000 dimensional feature from classification layer (fc1000) and 2,048 dimensional feature from final convolution layer (pool5). Each dimension of the fc1000 feature corresponds to a WordNet synset here.\n",
    "- features_test - features extracted from the same Residual Network (ResNet) on test set, including 1,000 dimensional feature from classification layer (fc1000) and 2,048 dimensional feature from final convolution layer (pool5).\n",
    "- descriptions_train - image descriptions correspond to training images. Each image have 5 sentences for describing the image content.\n",
    "- descriptions_test - image descriptions for test images. Each image have 5 sentences for describing the image content. Notice that one test description corresponds to one test image. The task you need to do is to return top 20 images in test set for each test description.\n",
    "- sample_submission.csv - a sample submission file in the correct format.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import pylab as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in training data\n",
    "\n",
    "# Sort files in ascending order\n",
    "def order_keys(text):\n",
    "    return int(text.split('.')[0])\n",
    "\n",
    "# Define paths\n",
    "my_path = os.getcwd()\n",
    "image_train_path = os.path.join(my_path, 'images_train')\n",
    "desc_train_path  = os.path.join(my_path, 'descriptions_train')\n",
    "tags_train_path  = os.path.join(my_path, 'tags_train')\n",
    "features_train_path = os.path.join(my_path, 'features_train')\n",
    "\n",
    "# Define arrays\n",
    "images_arr   = []\n",
    "desc_arr     = []\n",
    "tags_arr     = []\n",
    "features_arr = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the images\n",
    "image_files = os.listdir(image_train_path)\n",
    "image_files.sort(key = order_keys)\n",
    "for image_file in image_files:\n",
    "    # Open each image file\n",
    "    im = Image.open(os.path.join(image_train_path, image_file), 'r')\n",
    "    \n",
    "    # Convert to an np array\n",
    "    images_arr.append(np.asarray(im))\n",
    "\n",
    "    # Close the file\n",
    "    im.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the descriptions\n",
    "desc_files = os.listdir(desc_train_path)\n",
    "desc_files.sort(key = order_keys)\n",
    "for desc_file in desc_files:\n",
    "    # Open each text file. Strip leading/trailing whitespace\n",
    "    lines = [line.strip() for line in open(os.path.join(desc_train_path, desc_file))]\n",
    "    \n",
    "    # Convert to an np array\n",
    "    desc_arr.append(np.asarray(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the tags\n",
    "tag_files = os.listdir(tags_train_path)\n",
    "tag_files.sort(key = order_keys)\n",
    "for tag_file in tag_files:\n",
    "    # Open each text file. Strip leading/trailing whitespace\n",
    "    lines = [line.strip() for line in open(os.path.join(tags_train_path, tag_file))]\n",
    "    \n",
    "    # Convert to an np array\n",
    "    tags_arr.append(np.asarray(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the features\n",
    "feature_files = os.listdir(features_train_path)\n",
    "for feature_file in feature_files:\n",
    "    reader = csv.reader(open(os.path.join(features_train_path, feature_file)), delimiter=\",\")\n",
    "    sortedlist = sorted(reader, key = lambda row: int(row[0].split('/')[1].split('.')[0]))\n",
    "    features_arr = sortedlist\n",
    "\n",
    "# TODO - two diff lists\n",
    "#     features_resnet1000_test.csv\n",
    "#     features_resnet1000intermediate_test.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format features into pd dataframe\n",
    "# print len(images_arr)\n",
    "# print len(desc_arr)\n",
    "# print len(tags_arr)\n",
    "# print len(features_arr)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['images'] = images_arr\n",
    "df['descriptions'] = desc_arr\n",
    "df['tags'] = tags_arr\n",
    "df['features'] = features_arr\n",
    "\n",
    "print df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLD STUFF\n",
    "\n",
    "# Some of the training data did not have space after punctuation and was considered together as a word\n",
    "# To account for this, punctuations were replaced with a blankspace to be splitted later. The words in sentences were converted to lowercase as well\n",
    "# def fixpunct(s):\n",
    "#     new_sent = ''\n",
    "#     for i in s:\n",
    "#         if i not in string.punctuation:\n",
    "#             new_sent += i\n",
    "#         else:\n",
    "#             new_sent += ' '\n",
    "#     return new_sent.lower()\n",
    "\n",
    "\n",
    "# # Iterate through the training data and generate a list of unique words\n",
    "# uniquewords1 = []\n",
    "# lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "# for sentence in training_data1:\n",
    "#     sentencefixed = fixpunct(sentence)\n",
    "#     tokens = nltk.word_tokenize(sentencefixed)\n",
    "#     tokens = [lmtzr.lemmatize(word, 'v') for word in tokens]\n",
    "#     tokens = [word for word in tokens if word not in stopset]\n",
    "#     for word in tokens:\n",
    "#         if word not in uniquewords1:\n",
    "#             uniquewords1.append(word)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output results to CSV\n",
    "\n",
    "# with open('test_results.csv', 'w') as csvfile:\n",
    "#     writer = cimagessv.writer(csvfile, delimiter=',')\n",
    "#     writer.writerow(['Description_ID','Top_20_Image_IDs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rando notes\n",
    "\n",
    "# Preprocessing\n",
    "# Lowercase all of the words.\n",
    "# • Lemmatization of all the words (i.e., convert every word to its root so that all of “running,”\n",
    "# “run,” and “runs” are converted to “run” and and all of “good,” “well,” “better,” and “best”\n",
    "# are converted to “good”; this is easily done using nltk.stem).\n",
    "# • Strip punctuation.\n",
    "# • Strip the stop words, e.g., “the”, “and”, “or”.\n",
    "\n",
    "#Processing\n",
    "# bag of words, 2-gram and PCA for bag of words\n",
    "# - Common words like \"the\", \"a\", \"to\" are almost always the terms with highest frequency in the text. \n",
    "# Thus, having a high raw count does not necessarily mean that the corresponding word is more important. \n",
    "# To address this problem, one of the most popular ways to \"normalize\" the term frequencies is to weight \n",
    "#a term by the inverse of document frequency, or tf–idf.\n",
    "# https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering\n",
    "\n",
    "\n",
    "#Postprocessing\n",
    "# log-normalization\n",
    "# l1 normalization\n",
    "# l2 normalization\n",
    "# Standardize the data by subtracting the mean and dividing by the variance.\n",
    "\n",
    "\n",
    "# https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# maybe preprocessing / bag of words to match query with an image\n",
    "# then k-means on the feature vectors of images to find closely related images\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
